判断可行性：

1. 商务上是否有吸引力

2. 技术上有没有违反基本原理

   - 手机摄像头、GPU、type-c、4G 5G、电源均满足需求

   - 通过一系列方法逐步达成目标

     

- 绘制准确度 和道路贴合
- 演示内容：直行、定点显示、人行横道，slowdown
  - 道路信息：需要识别车在哪个车道上 算法来解决 数据源（视频/传感器），变道信息。车架上可以静态展示，实车没有走通
  - 车辆、行人的识别和追踪
  - 例如，距离右转弯还有多远开始绘制图标

车怎么知道自己的速度-前装方案，后装方案



##### ARHUD 关键技术研究现状

对于关键技术研究，我们目前三个主要方向：

- 车道线的检测（图像分割技术，已有一些结论）
- vo 视觉里程计（定位和建图，初步进行中）
- 地图（二维地图 or 实景地图，基于特征线/特征点的），目前无进展



##### 1、为什么"视觉里程计vo"单目摄像头是关键技术之一？

- 答：在视觉 SLAM 中，我们有两大基本问题：
  - 我在什么地方？——定位，自身状态，我们希望准确知道位置。
  - 周围长什么样子？——建图，外在环境【疑问：这里的建图，建立的就是视空间和世界空间吗】
  - 那么，**视觉里程计，解决的是定位的问题**。
- 先解答这样一个问题：在视觉里程计中，单目摄像头是如何工作，来估计自己的位置的？
  - 照片，以二维的形式记录了三维的世界，丢失了一个维度：深度，而这个维度是 SLAM 中非常关键的信息。
  - 如果想恢复三维的结构，必须移动相机，才能估计它的运动和结构（场景中物体的远近和大小）
  - 存在的问题：尺度不确定性
- 为什么要视觉里程计，要不要结合 gps，为什么？视觉里程计不需要结合 GPS，一方面是由于在 SLAM 中我们非常强调未知的环境，在特殊环境下，例如室内停车场，GPS无法正常工作，且即使是在有 GPS 的道路环境中，GPS 能够提供的信息的精度有限，无法提供我们需要的位置信息，至少无法精确到车道级别。因此，使用携带式的传感器来完成 SLAM 是我们重点关心的问题。特别地，当谈论视觉 SLAM 时，我们主要是指如何用相机解决定位和建图问题。
- 三维包括：道路前进方向（沿着路的方向）、横穿道路方向、竖直向上方向，不同之处是，我们对这三个方向上的误差精度的要求不一样。前进方向上的误差要求最低，希望算法输出的东西给工程的时候，工程可以画的比较准。定位问题的思考：是绝对的定位，还是相对的定位？应该是相对的定位。为什么是**相对的定位**？
  - 首先我们的目的是为了定量地估计相机运动，因此必须先了解**相机**与**空间点**的几何关系。

    - VO 能够通过相邻帧间的图像估计相机运动，并恢复场景的空间结构。它和实际的里程计一样，**只计算相邻时刻的运动**，而和再往前的过去的信息没有关联。就像一种只有短时间记忆的物种（可以不限于两帧，例如5-10帧）。
    - 现在，假定我们已估计了两张图像间的相机运动，只要把相邻时刻的运动“串”起来，就构成了机器人的运动轨迹，从而解决了**定位问题**。另一方面，我们根据每个时刻的相机位置，计算出各像素对应的空间点的位置，就得到了**地图**。
- 我们的输出是二维的，输入也是二维的，但是过程要基于三维重建，这样才能获得三维的一些信息，例如，车在踩刹车的时候，会停下来，这时候箭头也要停下来，就像刷在路上的车道那样。SFM与SLAM技术的区别在于，SLAM是实时的SFM，实时的三维重建，要实时的感知周围的环境。

##### 2、基于图像的全景分割技术

创建和更新的过程

- 转弯；前方的车和人（路不动人动，动的是前景，不动的是背景）

- 在右转弯路口，转弯时应该能标识右转箭头的位置（用户看到的）
  - 右转箭头、直行图标在世界空间是不动的。那么，我怎么知道图标应该显示在哪里？我需要知道自己的准确位置，这就用到了上述的视觉里程计。
  - 显示的时机是什么？导航API告诉的，我们可以假设这个时机不需要我们来确定。例如，前方路口请右转，这时候显示。
  - 显示在哪里？既然叫增强现实，首先要知道什么是现实，要放在合适的地方。“请走右侧高架，不要上坡”
  - 怎么更新的
- 什么叫全景分割？因为我们要分析前方的车和人。
  - 以路为代表的不动的东西，成为前景
  - 以车为代表的动的东西，成为背景

##### 问：右转箭头用代码实现流程？

输入：单目摄像头采集到的图像、地图的导航信息

“请走最右侧车道”，识别图像，找到最右侧车道，根据一定的规则（找到道路边线，这个实现并不重要）

##### 下一轮更新

第一次创建，坐标系是相对于自己的，从数学上来讲，更新是自己的坐标系在连续的变化，而世界空间的坐标系是不变的。需要不断的计算在当前t时刻视空间的坐标。目前来看，世界空间是不需要重建的，重建的是视空间。无人驾驶要求重建世界空间，为什么？考察一个图标的生命周期，一个图标从出现在视空间，到它从视空间消失（到我后面去了），这是局部的。看不见它的时候，他的生命周期就结束了。而自动驾驶不一样，看不到的东西也需要考虑，防止碰撞。重建的目的是，从t=0到任何一个时候，误差都是最优的。

无人驾驶为什么需要重建世界空间：考虑局部性，考察图标的生命周期

道路有其特殊之处：用户需求决定的 有其特殊挑战 识别道路的精确信息（西直门高架）

坐标系的更新取决于vo，创建取决于图像分割

下周任务：基于仿真器

##### 3、地图构建和重定位技术

- 走一样的路，希望可以用更低的成本：闭环检测
- 问题：存在哪里？



下周：VO仿真器怎么验证，@wangyingrui

##### VO 和 图像分割 之间的依赖关系是什么样的？

VO 和 图像分割 是两个独立的技术，如果在最后的产品级别，应该是先后的顺序关系，是 pipeline 的形式，先分割，后 vo。分割决定了AR 的图标创建在哪里，通常是和世界空间的坐标系相关的（或者是另外某一个绝对的坐标系相关的），创建之后，在消失之前，通过 vo 来更新它在屏幕上的位置，因为在这个过程中，车的位置在动，图标的位置没动。

还有一种直接的方法是，通过分割+追踪来显示图标，不需要vo，因为在视空间中，行人/前车不是静止的，两个运动是独立的。前车是运动的时候，可以认为我不动，前车在动。前面有两辆车，两辆车运动的时候，他们是独立的。而对于路上的图标，不能够用认为“人行横道和车道线两个东西是完全独立的“这种方式来做，因为他们运动的自由度不会因为个体的增多而自由运动，路牌和车道线互相之间是相对静止的，因此两个图标在相机上面是静止的，他们在运动的时候无约束的自由度是在一个平面上运动的。

##### 世界空间和视空间的建立，和 vo 的关系？

1. vo 得到的是**视空间**的相对关系

   ARHUD 是否需要建立世界空间？无人驾驶需要，ARHUD是否需要，需要设计一系列实验来验证。从工程师的角度，考虑到工程的复杂程度，他们肯定不希望引入世界空间的坐标，需要我们考虑的是，如果不引入世界空间的坐标，会不会违反基本原理？如果没有违反基本原理，当然会希望你只建立视空间。

2. 图像分割得到的也是视空间的信息

   摄像头看到的全部都是是视空间，它没有办法提供世界空间的信息



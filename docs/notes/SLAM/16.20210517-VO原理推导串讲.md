# VO原理推导串讲

- 草稿：右转箭头流程
- DF-VO属于vo论文中的哪一种

思路：方案是什么，遇到的问题是怎么产生的，如何解决的，解决效果如何



#### 为什么需要 vo？在我们的项目中 vo 能起到什么作用？

为了方便理解，我们举个例子，比如说导航 API 告诉我们“请走最右侧车道”，这时候如何生成右转的箭头呢？

要想生成右转的箭头的话，这个过程可以被分为几个步骤：（流程图）

<img src="../../images/03073.png" alt="03073" style="zoom: 33%;" />

<img src="../../images/03073_1.jpg" alt="03073_1" style="zoom:33%;" />

1. 输入：单目摄像头采集到的图像（用一张采集图举例）
2. 识别图像，通过图像分割，找到最右侧车道，根据一定规则（比如说找到路边线、车道线）找到AR的图标应该创建在哪里，创建图标位置，并创建图标
3. 图标创建之后，在图标移动到视野之外之前，因为车在移动，所以需要不停的更新图标的位置。更新的时候，相对于地面来说，车的位置是动的，图标的位置是不动的，例如上面这张示意图，黄色的是右转图标。右转图标在创建的时候，创建在这个路标旁边的位置，那么当我们的车继续向前行驶的时候，比如从当前位置行驶到这条黄色虚线的这段距离，是要保持右转箭头的位置不变的，也就是说右转箭头始终相对地面静止，随着车的前进**看上去**会离车越来越近。那这个时候，车和图标它们有一个相对的运动，要计算出图标应该显示的位置。就需要我们重新建立视空间的坐标系，需要用到 vo。vo 用来估计车的相对运动，以及车的姿态。

下面来具体说一下 vo。

#### 什么是 vo？vo 的原理？

通过相邻的两帧，估计运动，获得位姿。

视觉里程计关心相邻图像之间的相机运动，最简单的情况当然是两张图像之间的运动关系。例如，当看到后两张图时，我们会自然地反应出右图应该是左图向→旋转一定角度的结果（在视频情况下感觉会更加自然）。我们不妨思考一下：自己是怎么知道“向左旋转”这件事情的呢？人类早已习惯于用眼睛探索世界，估计自己的位置，但又往往难以用理性的语言描述我们的直觉。看到图2-8时，我们会自然地认为，这个场景中离我们近的是人行道的红绿灯，远处是树和楼房。当相机向右转动时，红绿灯在图像中移到了左边，楼房离我们近的部分出现在视野中，而左侧远处的树则移出了视野。通过这些信息，我们判断相机应该是向左旋转了。

但是，如果进一步问：能否确定旋转了多少度，平移了多少厘米？我们就很难给出一个确切的答案了。因为我们的直觉对这些具体的数字并不敏感。但是，在计算机中，又必须精确地测量这段运动信息。所以我们要问：计算机是如何通过图像确定相机的运动的呢？

图像在计算机里只是一个数值矩阵。这个矩阵里表达着什么东西，计算机毫无概念（这也正是现在机器学习要解决的问题）。而在视觉SLAM 中，我们只能看到一个个像素，知道它们是某些空间点在相机的成像平面上投影的结果。

当我们已经能够估计了两张图像间的相机运动之后，只要把相邻时刻的运动“串”起来，就构成了相机的运动轨迹，从而解决了定位问题。

<img src="../../images/05480.png" alt="05480" style="zoom:33%;" />

<img src="../../images/05648.png" alt="05648" style="zoom:33%;" />

<img src="../../images/image-20210517105024003.png" alt="image-20210517105024003" style="zoom:33%;" />

<img src="../../images/image-20210517103458200.png" alt="image-20210517103458200" style="zoom: 33%;" />



##### 传统的 vo 算法是什么样的

书上的解释（不包括数学原理）：因为 vo 的核心是根据图像估计相机运动

讲一下传统 vo 算法与 智能 vo 算法的区别 就可以结合论文了（不讲细 细节拆下周工作量）

#### df-vo 是哪种 vo？大致原理？效果如何？进展？



#### 之前没讲的相机内参？

5.1 相机模型
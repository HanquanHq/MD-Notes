# 周周星分享

#### 05-22 0.63 baseline：基于DeepCTR实现的多任务学习模型MMOE

https://developers.weixin.qq.com/community/minihome/article/doc/000e40f8f1c3b8f9ea2c0f81551813

https://github.com/zanshuxun/WeChat_Big_Data_Challenge_DeepCTR_baseline

> 关于：什么是DeepCTR模型？ -- 面试中最常被问到的DeepCTR模型就是DeepFM模型
>
> [DeepCTR](https://github.com/shenweichen/DeepCTR)是一个易用、可扩展的深度学习点击率预测算法包，基于tensorflow深度学习框架。

```
13104/13104 [==============================] - 227s 17ms/step - loss: 0.3538 - prediction_layer_loss: 0.1530 - prediction_layer_1_loss: 0.1219 - prediction_layer_2_loss: 0.0508 - prediction_layer_3_loss: 0.0282
{'read_comment': 0.5024810778125054, 'like': 0.48282185157497914, 'click_avatar': 0.39537410201425993, 'forward': 0.4760492966272557}
Weighted uAUC:  0.472519
4个目标行为421985条样本预测耗时（毫秒）：123.852
4个目标行为2000条样本平均预测耗时（毫秒）：0.587

ROC AUC有个非常直白的概率意义。http://sofasofa.io/forum_main_post.php?postid=1001008
随机挑选一个标签为0的样本A，再随机挑选一个标签为1的样本B。你预测样本B为1的概率大于样本A为1的概率的概率就是你的ROC AUC。比较拗口，多念几遍，就通顺了，嘿嘿嘿。
我再多说几句，ROC AUC的数值与每个预测概率的数值大小无关，在乎的是每个预测概率的排序。假设我们按照概率从大到小排。如果根据你的预测结果，所有标签为1的样本都排在了标签为0的样本前面，那么你的ROC AUC就是1。
ROC AUC = 0.8的意思是说，随机挑选个标签为1的样本，它被排在随机的0样本的前面的概率是0.8。显然ROC AUC是0.5的话，就说明这个模型和随便猜没什么两样。
```



#### 05-30 集成多个baseline优势，重新组织代码，方便对比lgb/nn以及做额外特征

https://developers.weixin.qq.com/community/minihome/article/doc/000ea08c2e4b4008893c0a64251c13

eval_ratio可以调为不是0（eval_ratio是什么？）

deepFM可以到645+，就是波动真到很大。得用全部数据哦，不能采样，因为严重依赖id特征，采样完会掉很多分



#### 5.31 周周星（第二名）分享

1. 这个比赛正负样本不平衡，虽然总样本有七百万，其实正样本并不多，所以全量训练提升有比较大的提升；

2. 这个比赛有高维category数据，因此lgb设置比较低的学习率也可以带来比较好的提升；

3. 一定要在线下设置好验证策略，由于视频号这个业务的特殊性，存在一些ctr比赛常见套路特征在这个比赛会降低模型性能，但是这些都可以通过好的线下验证过滤掉，此外test数量不够多以及正样本比例小，榜上±二个千是正常的；

4. 关于模型之争，初赛有于正样本数量有限，我估计树模型会优于nn，但是复赛特征数量提升一个量级，我认为nn会优于lgb，特别是list以及多模态特征有包含不少信息量的情况，如果目标是复赛获奖的话，还是要努力提升nn模型。



#### 5.31 周周星（第一名）分享

之前看群里一直在讨论使用神经网络，也看到了有大佬分享的pytorch的baseline。但是本人对pytorch不太熟悉，于是自己也从github上找到一份tensorflow的deepfm开源，使用了几个id特征和id序列特征，在最后一天上做验证，跑出来线下0.643左右的结果，提交了一下线上0.637的分数，和开源的分数差不多，说明复现成功了。

然后我又相同的代码重新跑了一下，什么鬼，怎么就只有0.637了，神经网络波动怎么这么大。我不信邪，一口气又跑了好多遍，这里面有0.642，有0.641，最高的居然达到了0.647。哈哈，真是力大出奇迹。

想着既然这些分数波动这么大，我如果把他们**取平均**一下，应该会稳定很多。最终我把这八个模型等权取平均了一下，线上居然有0.653的分数！

后来群里有人说让试验**lightgbm**，提示说不要像deepfm那样把id直接输入给lightgbm。

> ##### lightgbm 是什么？参考：https://zhuanlan.zhihu.com/p/99069186
>
> GBDT (Gradient Boosting Decision Tree) 是机器学习中一个长盛不衰的模型，其主要思想是利用弱分类器（**决策树**）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。
>
> **LightGBM**（Light Gradient Boosting Machine）是一个**实现GBDT算法**的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。
>
> 在LightGBM提出之前，最有名的GBDT工具就是XGBoost了

根据提示，我对各种id以及交互id统计了一系列的交互数量，以及转化率等1000个左右的特征，庆幸自己有一台还不错的机器，不然还真跑不动。目前自己的lightgbm模型线上已经达到0.668分左右。

目前榜上的分数是我融合了lightgbm和deepfm之后的分数。我相信前排真正的大佬们还没开始**融合**，所以还是有投机取巧的成分，期待以后真正的大佬的分享吧。大家加油。

> ##### 模型的“融合”是什么意思？对结果取平均吗？有哪些融合方式？
>
> ##### 一、Voting
>
> 模型融合其实也没有想象的那么高大上，从最简单的Voting说起，这也可以说是一种模型融合。假设对于一个二分类问题，有3个基础模型，那么就采取投票制的方法，投票多者确定为最终的分类。
>
> ##### 二、Averaging
>
> 对于回归问题，一个简单直接的思路是取平均。稍稍改进的方法是进行加权平均。权值可以用排序的方法确定，举个例子，比如A、B、C三种基本模型，模型效果进行排名，假设排名分别是1，2，3，那么给这三个模型赋予的权值分别是3/6、2/6、1/6
> 这两种方法看似简单，其实后面的高级算法也可以说是基于此而产生的，Bagging或者Boosting都是一种把许多弱分类器这样融合成强分类器的思想。



#### 5.31幸运周周星（第六名）分享

1. 设置良好的线下验证集（前十三天训练，第十四天验证是一个常规但有效的策略），线上线下的差距（使用评价指标uAUC）越小越好，这样无需提交就可以知道自己达到了一个什么样的水平；

2. 最好不构造穿越特征、全局统计；

3. 不要盲从，有人说NN强就用NN，lgb强就用lgb，要选择自己熟悉的模型以及自己判断认为合适于本赛题的模型来进行特征构建、模型训练以及最终的模型融合；

4. 在特征上做减法，不要一顿梭哈，根据对视频号任务的理解来做特征往往是非常有效的；



#### 6.7周周星（第一名）分享

本人目前使用的是Catboost单模型加自己的手工特征，特征量在300+，建议选手多看看数据，而不是一股脑将数据强行喂入模型。

> ##### catboost 是什么？
>
> CatBoost是一种基于对称决策树（oblivious trees）为基学习器实现的参数较少、支持类别型变量和高准确性的GBDT框架，主要解决的痛点是高效合理地处理类别型特征。
>
> CatBoost是俄罗斯的搜索巨头Yandex在2017年开源的机器学习库，是Boosting族算法的一种。CatBoost和XGBoost、LightGBM并称为GBDT的三大主流神器，都是在GBDT算法框架下的一种改进实现。

具体可分享的如下：

1、目前的初赛数据我猜测是复赛的一个子集，但采样的时候导致有的feed序列出现不连续的情况，比如1、2、4、5天都有被用户浏览过，但date缺失了3，

如果选手是用深度模型的话，可能会受到影响，具体可自己实验分析;

2、很多选手做的手工特征加到模型后，验证集取得了较大的提升，线上却gg，这种大概率是穿越了，具体穿越的原因可自己进行分析，有的穿越还是不容易被发现的;

3、树模型的潜力可能没有深度模型大，目前树模型取得的优势很大程度上在于初赛的数据不够大，不过为了进入复赛，树模型还是有可研究的价值的。具体可做的特征不仅仅

是LabelEncoder，One-hot，还有TargetEncoding，TFIDF等。

树模型类似比赛比较好的开源代码我这边例举一些：

https://github.com/plantsgo/ijcai-2018

https://github.com/YouChouNoBB/2018-tencent-ad-competition-baseline

https://github.com/digix2020/digix2020_ctr_rank1

最后建议选手们多多试错，多加思考，多从业务上理解数据。



#### 6.7周周星（第二名）分享

1. 同一天内的数据存在较多泄露，因此用随机交叉验证会导致线下分数虚高，按日期来切割线下的验证集是比较稳定的做法。线上测试集中的user都在训练集中出现过，feed有一些没出现过，user-feed对都没出现过，同一个user每一个feed最多出现一次，等等。构造验证集时要考虑是否要做一些筛选，使得测试集和验证集的特性尽量接近

2. 少量行为用户比较难预测，会导致uauc波动大，因此加某个特征上分了不一定说明这个特征有用 可能只是波动

3. play,stay这些信息在测试数据中没有，因此不能直接用来训练，但是可以用来提取一些feed的相关特征

4. 图神经网络是我最近一直在研究的算法，据说该算法在推荐系统中大有作为，因此觉得以此算法为队名比较吉利，希望大家不要被误导。



#### 6.7周周星（第四名）分享

大家好我是来自challengehub的wintomt，目前使用的模型是之前开源的nnbase，特征100不到.

1.建议选手不要疯狂造特征一把梭，适当做些特征减法对上分有奇效.

2.有些理论上可行的特征加进去线上效果不好，排除掉选手个人问题，更多的则是反应数据集的一些问题，所以无效特征不一定完全没有价值，多分析分析这些无效特征无效的原因，反而能帮助我们上分

3.这题很玄学，参数很重要



#### 6.14周周星（第一名）分享

1、目前分数是融的，前排大佬们不要担心了。

2、目前树模型单模0.675左右，跟前排0.68的单模还是有一定的差距，很佩服单模那么高分的。

3、个人认为树模型在复赛很难搞，没有很深的树模型相关经验的选手，建议集中注意力研究神经网络。

4、对于我这边的树模型来说，**最有用的只有userid、feedid、authorid三个id列，估计80%以上的收益都来自这三列**，bgm只有一丁点用，诸如**description、ocr等文本信息几乎没啥用**，**keyword、tag等信息有一点点用**，认真研究三个主要id才能拿下这个数据里面绝大部分收益。当然那些没啥用的字段也可能是我没找对用的方法，希望会用的大佬可以透露点信息。

5、树模型的特征具体怎么做，我也不多说废话了，其实大佬们已经说的够多了，随便找以前比赛的树模型开源方案看看也会做了，但是我估计真的去找来看的也没几个，大部分人也只会在群里呻吟：onehot没效果、labelencode没效果之类的话。我在这里**开源一份lightgbm的baseline**，里面包含一些简单的特征，希望可以启发一下新手们吧。

代码：https://github.com/HanquanHq/MD-Notes/blob/master/docs/notes/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/lightgbm_baseline.py



#### 6.14周周星（第二名）分享

2. 关于比赛经验的分享：真的是小白，这次能到前排很大程度是运气好。

  2.0 分数高的原因和第一名相同，也是融的。

  2.1 模型上我用的基于mmoe的nn模型，目前单模单折能到0.671左右（需要靠线上试最好的epoch），K折融合之后提升非常大，能到0.678+，但是这个K呢需要大家自己去试，就不透露了。

  2.2 特征工程这块儿感觉没什么心意，都是之前比赛开源的里面摘的一些。之前大佬的分享里有三个开源的，里面有一些很有用的东西，建议仔细学习代码。其实特征工程这块儿还是有很多东西可以做的，尤其是使用树模型的选手，可能比较依赖这个，而且构造的dense特征怎么和nn模型结合起来发挥更大的作用，也是nn选手需要尝试的，我也在探索中。

  2.3 我还尝试了把mmoe中的gate换成attention，有提升，大家也可以尝试一下。

  2.4 多任务学习有个非常难的地方就是不同任务之间的平衡，目前的权重是手动设置的（靠试），也尝试了其他调整loss的方法，收益不大。

  2.4 nn的波动非常大，同样方式构造的不同种子的两份数据，线上效果可能会差个0.002，但是直接取平均收益就会很大。

  2.5 正样本非常稀疏，如何采样让nn能学到东西非常非常关键。

能和各位大佬们同台竞技非常荣幸，如有讲的不对的地方还请大家多多指正，祝大家多多上分！



#### 6.14周周星（第四名）分享

目前主要用lgb和nn融合。

lgb我用了500多个特征，**主要包括device以及feed, author, bgm_song, bgm_singer, keyword和tag特征，其中keyword和tag统计后取平均值。我用历史前7天统计，后7天来训练。**主要统计7个标签的count和比例，以及stay和play的min, max, mean, median, std等统计值。此外加入512维embedding的pca降维特征也有一定提升。而user特征几乎没有作用。有一些小发现，比如测试集user-feed对都是新出现的，比如测试集可能是周四，比如click_avatar和follow间的相关性很强，这些都没能帮我提高分数。

nn我用的是deepctr，直接输入上面的id训练。但是deepctr不太稳定。我**训练了10份结果并取了均值**，相对于单模型有比较大的提升。

目前我**lgb能做到0.672左右，和nn融合后达到0.68**，但是应该有很大的overfitting，换榜后大概会显著变差，估计前排其它队伍可能也有一定程度的overfitting。



#### 微信视频号推荐算法解题思路

本文尝试从更宏观的角度去设计这个推荐算法的模型训练思路，其中涉及到了数据的划分，整体特征工程的框架，方法论，在模型训练时尝试子模型结构，用于处理潜在的稀疏数据，在模型架构方面，对于以树模型为核心的模型，设计了两阶段模型，对于深度模型，尝试分视频种类，利用注意力机制学习目标视频与历史视频的权重，充分利用多模态信息；

https://mp.weixin.qq.com/s/yE5yThqZ8R9v4EIxlr3bsA



#### 6.21周周星（第三名）分享

##### nn :

1. 模型上采用的是dcn和deepfm的融合

2. 特征主要就是id特征、文本特征、feed_embedding特征和w2v特征

3. 采用5折交叉验证可以上分

##### 树模型

1. 除了统计特征外，和nn基本使用同一套特征

主要上分点：模型融合



#### 6.21周周星（第六名）分享

##### nn模型：

1、目前nn使用的是开源的baselline：mmoe模型。训练策略：线下使用前十三天训练，第十四天作为验证，线上利用前十四天数据作为训练。一般线上比线下高0.002~0.005。

2、模型的输入主要包括五个id、videoplayseconds、利用PCA对feed_embedding降维的64维向量，构造序列训练的向量等、以及tag作为输入，tag处理方式截取前n个tag不足补0,当作文本输入，经过Embedding层后与id类特征的Embedding拼接。

3、由于tensorflow不太稳定，训练三次取平均得到现在的成绩。也尝试过online和offline两次的预测结果加权平均，提升没有三次online预测结果取平均好。

4、目前在尝试前面周周星分享的多折策略，目前使用全量数据**5折**，线下是有一些提升，测试集由于时间原因还未提交。

##### 树模型：

1、对于树模型来说，用到的还是几个常规特征加embedding，比如**用户前七天的平均点击量**等统计特征。构造序列时我是使用了userid与所有广告侧id的交互。特征中**文本和ocr等我目前试没什么效果**，**tag，keyword和feed_emb总共大概有一个百分点**的提升, tag和keyword的用法我是**滑窗和直接暴力拼接**。

2、树模型的多折策略我是采用时间均匀划分，将负样本加入划分数据，每次选择划分下来的验证集加入训练数据中，与正样本拼接起来构成最后的训练数据，多次后所有的负样本都参与了训练。

目前分数是**nn和树模型融合**的分数。

> 比赛中常说的模型融合是什么意思？
>
> https://zhuanlan.zhihu.com/p/40131797 就是把不同模型的预测结果，以一定的规则进行综合。



#### 6.21周周星（第八名）分享

##### 树模型

1、目前我这边的树模型并没有加入两个bgm（背景音乐）相关id的任何特征，只用了**另外3个id的历史统计，交叉，embedding等**前面大佬们分享过的特征

2、**feed表内的三种文本**和**tag**都是有效的提分手段，幅度在1个百分点左右，tag的话我是直接取概率最高的当类别做统计（当然还可以有别的方式值得尝试），keyword还没开始研究，这部分可以看看历史ctr比赛的代码

3、**不同树模型融合**能提分，**单树模型K折**也能提分，但总分提升幅度不大，特别是后两个小项几乎没变

> 模型K折什么意思？https://zhuanlan.zhihu.com/p/24825503
>
> LOOCV：它不受测试集合训练集划分方法的影响，因为每一个数据都单独的做过测试集。同时，其用了n-1个数据训练模型，也几乎用到了所有的数据，保证了模型的bias更小。不过LOOCV的缺点也很明显，那就是计算量过于大，是test set approach耗时的n-1倍。
>
> K-fold Cross Validation：另外一种折中的办法叫做K折交叉验证，和LOOCV的不同在于，我们每次的测试集将不再只包含一个数据，而是多个，具体数目将根据K的选取决定。比如，如果K=5，那么我们利用五折交叉验证的步骤就是：
>
> 1.将所有数据集分成5份
>
> 2.不重复地每次取其中一份做测试集，用其他四份做训练集训练模型，之后计算该模型在测试集上的MSEi
>
> 3.将5次的MSEi取平均得到最后的MSE

4、即使是滑窗也可以把前面几天的数据加上训练，说不定会有惊喜

##### 神经网络

1、线下使用前十三天训练，第十四天作为验证，线上利用前十四天数据作为训练

2、**四个子任务收敛不同步**，**拆分模型**能在mmoe的基础上提几个千

3、nn**多训练几个取平均**之后，模型效果会稳定很多



#### 分享：微信大数据比赛我用树模型怎么从514到671

链接：[微信大数据比赛我用树模型怎么从514到671](https://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&mid=2247487140&idx=1&sn=f7c29483b01db53447d60a7b650dd7ff&chksm=9bc5e6f6acb26fe06abeb9ff81142ab862087a6fc4ed83de73dfcbb9a8f2998cc405969114bd&mpshare=1&scene=23&srcid=0618UgvVlPuYzt2j3FSbI7DZ&sharer_sharetime=1623948917125&sharer_shareid=ef2ccb5ab40f6a6c7a2810846899b016#rd)

##### 第一阶段：统计类特征

主要针对几个id做了一些常规的统计特征，比如**某个user今天查看了多少次**等等

注意：本题中虽然训练集是按照时间戳排序的，但是测试集是完全打乱的，所以别自作聪明地做rank类特征。

采用几个id做单纯的统计特征分数特别低！千万别执着于此。

##### 第二阶段：label滑窗类特征

这明显是一个很不错的方向，于是我在两天时间内迅速到达了64。但是很遗憾的是它只让我达到了64，根据周星星开源的代码，做了滑窗类特征之后，也确实只有0.64+，但是我当时并不是知道，花了几天的时间调各种滑窗，虽然不知道最后的分数是多少，但是我记得并没有到达0.65。

所以目前滑窗特征做到0.64+的选手可以不必太纠结于那里了。可以下一步的方向了。那么下一步的方向是什么呢？那就是tag和keyword。

##### 第三阶段：tag和keyword

在短暂的几天调滑窗之后，我将tag和keyword降入到了特征中，方法有两种，第一仍然是滑窗，第二是当作类别类特征做统计特征。具体的详细操作大家可以找找类似的比赛，或者问问人。可以肯定的是加入了tag和keyword之后，分数的提升非常大，直接跃到了65+，最后我的成绩是657，觉得无法提升之后，我进入了下一步。

> 你把tag当做feedid，该咋滑咋滑，或者行转列，滑完后pooling
>
> 先只加1列tag看效果(0.643->0.648) 然后再加多列 或者加keyword

##### 第四步：embedding特征

万物皆可embedding诚不欺我，当我做到657的时候，不少人通过nn已经做到了67，我队友当时也做到668，我就在思考，为什么nn可以轻易的做到那么高的分数，从目前的分数情况来看，单独的nn的结果也是要比树模型的结果要好的。

**nn最善于的就是embedding**，还有之前很多推荐比赛中也有不少人用到了embedding的方法，于是我去用了，结果大家也看到了，确实可以很直观的上分吗。**加入了一个w2v embedding之后成绩迅速到达了66**，但是当我继续加的时候，发现不怎么上分了。

我在66呆的时间特别长，因为一直在探索新的方法和一些有效无效的尝试，主要就是调整embedding的维度，用更多的embedding，其实这些操作也能带来一点提升，但是很小，有波动，这也就是我为什么一直停留在66了。但是前几天，我又开始了新的探索。

##### 第五步：基于embedding的特征

这一步是可以达到67的，我就不说的很直白了。前面的那些我感觉有手就行，在天才儿童的645baseline上面，很容易660的。第五步是基于第四步做的，采用了第四步得到的一些embedding去做一些特征，大家可以看下DIN模型，没准会有点启发，就是用别的推荐的思想去构建构建特征(做这方面的特征会比较复杂)。还有可以做一些类似于itemcf类的特征(这个我也在尝试中)。

以上就是我的上分的历程，主要是五个阶段，大家可以采用开源的baseline从第三步开始，希望大家早日超越我。

##### 接下来的方向：

1：接下来的话继续完善第五步的特征，还有就是由于历史遗留问题，特征已经突破了500个，肯定要删除一些没啥用的特征。2：采用多折，或者嫁接等方法(不过初赛不推荐使用，主要是没必要)

##### 接下来说点小方法：

1：**baseline中的read_mean（reduce_mem？）一定要用**，这样的话32G内存可以做400+特征

2：组件化特征，特征离线都计算好，这样的话增删特征方便，并且验证起来速度快，只需要拼接特征就好了。

3：图神经网络确实可以上分！



#### 6.27周周星（第八名）分享

很荣幸以学生队的身份获得了最后一周的周星星。

nn模型用的是最简单的mlp，隐层的参数要调节一下，多任务用的mmoe的结构，但我们这边mmoe的提升其实不大，可能是因为mmoe的参数我们并没有怎么调节。

特征的话前几周的周星星全说过了，大家看一下前几周的周星星就知道了。

融合了队友的树模型达到的目前榜上分数。

主要上分点：模型融合

现在初赛100名的线大概在67+，其实开源的baseline大家魔改一下就能66了，如果不能的话可以仔细检查一下代码是不是哪里有bug，或者是不是比如哪里的函数设置的不合理。



#### 6.27周周星（第一名）分享

很荣幸，获得最后一周的周星星。大家很关注NN，本分享更多关于NN的尝试。

1. 目前分数是三个队友的融合结果，融合提升最大的是树模型和NN，所以我们这次的提升的原因是树模型分数上分了。

2. 深度NN方面。因为之前看过类似的论文，所以优先使用了如LSTUR模型以及NRMS模型等multi-head attention的策略，一波梭哈，大概在0.671左右。后面继续深挖，竟然毫无提升。进一步分析有可能是bgm的缺失导致，试过将所有的缺失为唯一编码，以及每个缺失单独一个编码（怕和当前的unk match），分数略有提升。进一步的，某些类似于din（懂得人应该明白是什么特征）的相关特征在树模型里面特征重要性很高，因此我在NN中单独设计了具有权重衰减单独的结构（兼顾feedid视频位置信息和日期信息），用来学习用户长短兴趣变化转移，带来线上收益很大。除此之外，前面周星星分享MMOE结构模型也可以到0.678，而且推理时间短。最近两周我对MMOE进行魔改，尝试过FM+MMOE、CIN+MMOE、LSTM+MMOE、增加共享和专家网络、multi-head结构、时间权重衰减以及自适应loss权重等方式，有些魔改一定提升，大家可以自行尝试，最终单模单折680+。此外，也可以进行调参练丹，比如batchsize、emb的大小等等。

3. 融合方面。模型融合的方式有很多。本场比赛我采用的是简单的加权平均。大家可以试试。同时可以按照线上每一项的实际分数（比如：NN的read较高，like较低，树模型刚好相反）进行加权平均，也会带来1到2k左右的提升。融合也是需要不断测试，门道很多，毕竟大家目前差距也就千分位。



### 其他

#### pd.concat example

```python
>>> df1 = pd.DataFrame({'A': ['A0', 'A0', 'A2', 'A2', 'A2'],
...                         'B': ['B0', 'B1', 'B2', 'B3', 'B3'],
...                         'C': ['D0', 'D1', 'D2', 'D3', 'D4']},
...                         index=[0,2,3,4,8])

>>> df1
    A   B   C
0  A0  B0  D0
2  A0  B1  D1
3  A2  B2  D2
4  A2  B3  D3
8  A2  B3  D4

>>> df1['count_A_A']=df1.groupby('A')['A'].transform('count')
>>> df1['count_A_B']=df1.groupby('A')['B'].transform('count')
>>> df1['count_A_C']=df1.groupby('A')['C'].transform('count')
>>> df1['nunique_A_A']=df1.groupby('A')['A'].transform('nunique')
>>> df1['nunique_A_B']=df1.groupby('A')['B'].transform('nunique')
>>> df1['nunique_A_C']=df1.groupby('A')['C'].transform('nunique')

>>> df1
    A   B   C  count_A_A  count_A_B  count_A_C  nunique_A_A  nunique_A_B  nunique_A_C
0  A0  B0  D0          2          2          2            1            2            2
2  A0  B1  D1          2          2          2            1            2            2
3  A2  B2  D2          3          3          3            1            2            3
4  A2  B3  D3          3          3          3            1            2            3
8  A2  B3  D4          3          3          3            1            2            3


>>> df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],
                        'B': ['C4', 'C5', 'C6', 'C7'],
                        'C': ['D4', 'D5', 'D6', 'D7']},
                        index=[ 5, 6, 7,3])

>>> print(df2)
    A   B   C
5  A4  C4  D4
6  A5  C5  D5
7  A6  C6  D6
3  A7  C7  D7

>>> pd.concat( [df1,df2],axis=0,ignore_index=True)
    A   B   C  count_A_A  count_A_B  count_A_C  nunique_A_A  nunique_A_B  nunique_A_C
0  A0  B0  D0        2.0        2.0        2.0          1.0          2.0          2.0
1  A0  B1  D1        2.0        2.0        2.0          1.0          2.0          2.0
2  A2  B2  D2        3.0        3.0        3.0          1.0          2.0          3.0
3  A2  B3  D3        3.0        3.0        3.0          1.0          2.0          3.0
4  A2  B3  D4        3.0        3.0        3.0          1.0          2.0          3.0
5  A4  C4  D4        NaN        NaN        NaN          NaN          NaN          NaN
6  A5  C5  D5        NaN        NaN        NaN          NaN          NaN          NaN
7  A6  C6  D6        NaN        NaN        NaN          NaN          NaN          NaN
8  A7  C7  D7        NaN        NaN        NaN          NaN          NaN          NaN
```

